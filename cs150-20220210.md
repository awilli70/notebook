# CS 150 Class Notes 2022/02/10
## Guest lecture: Peter Portante at Red Hat
## Let's Fix Logging Once and for All
### From P99 Conf

Modifying the Kernel to solve the logging problem when bandwidth is hit.
Also want to provide a way for the service owner to control what happens.
For cloud systems, this is critical and not really solved

Terms:
- SRE: site reliability engineer/ sysadmin
  - Need control over system behavior and combined bandwidth on a node in a
    distributed system

Containers prevalent in cloud computing as an object to be reasoned about

* X size, bandwidth capability, etc.

Ideal: Big system built from smaller boxes with schedulers coordinating.
Container orchestrators wonderful until you start to use and operate them.
Logging system undermines the system because it can write logs completely
unbounded. (Dumping logs whenever, kernel has to manage all the log data) - but
kernel can not signal when it's getting overloaded.

- Logs are coming from everywhere, not just within the containers 
  (TOTAL BANDWIDTH)

OS responsible for preventing destabilization from a process

Somebody is responsible when a service goes down, so we need to solve this
problem such that the SRE can maintain control

#### Goals:
SRE should be able to restore behavioral control for logging. (READ: SRE
should be able to set maximum bandwidth)

Applications should stay in control when the limits are hit/ remain stable.
(READ: Applications need to prepare. Do they rate limit, or drop logs?)

* Options: stderr only, keep both, drop all, etc

Want application to be in control of application part,
SRE to be in control of system part

Implement an opt-in bandwidth gate

* SRE sets bandwith limit (system-wide, amt per interval)
* `write()` writes to a file descriptor. Ensure that it does not move the data
  when bandwidth limit is hit during interval

Application Policy
(SRE sets default)

* drop
* block

Match policy at the time of use in the container runtime - but all fds by
container must be considered

Why is this a problem?

* **Fundamental Cause:** Containers in cloud systems where the "fleet" is 
  managed like "cattle" Container runtime interface uses bytecapture and 
  interpretation of stdout/stderr written to disk first 
* Hardware more dense, increasing number of applications that are able to
  generate more log data than disk and network bandwidth available
* Separation of SRE from App developer (containers make building and running
  anywhere easy) (And for that point, also the end user of the container)

#### Container Logging in conmon

1. Container (abstraction) - There is a container monitor process conmon
   Containers pipe stdout and stderr to conmon ahead of time so the container
   can spawn a process. conmon attaches to the process through the stdout and
   stderr pipes. Pipes at 64kb in size, and conmon sends the logs to
   some container.log in the logging disk
1. Container my have n different processes all running in the same container,
   which inherit the pipes set up earlier
1. With every process, there may be m threads which all write to stdout/stderr
   (inherited from container). **THERE COULD BE THOUSANDS OF THREADS, ALL WRITING
   TO THE SAME LOCATION**
1. But there isn't just 1 container, there are j containers, with 2j pipes, and
   all of them write to j conmons, writing to j files on the disk through 
   `write()`. Disks are finite, and write has a finite available bandwith
1. And now, the data also has to be read from the disk, and sent off-host
   over network to a log scraper - but this log scraper has to keep up!
   So: log scraper can't keep up, filling container logs, blocking at pipes

#### Solution in Kernel
*Challenge* find an os that allows apps in userspace to write data - because
doing so is patently insane

* Becuase most of the problem is occurring in the system calls
* Berkeley Packet Filter has been extended and added to the linux kernel
  (Reading: Look up BPF performance tools)

Implement solution between conmon and the container.log - before writing,
check bandwidth limit and apply policy described for application.
Problem: All data is still being written into the pipes with block.
Why drop between conmon and container, when we want this to be done at
the application level? Want application to know policy when the pipe fills up.

eBPF wants to be at the front of the file descriptor, rather than being at the
backend (conmon read/write to log).
